2016-09-04 23:15:57,089 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-09-04 23:15:57,102 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-09-04 23:15:57,106 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-09-04 23:15:57,626 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-09-04 23:15:57,825 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-09-04 23:15:57,825 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-09-04 23:15:57,828 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-09-04 23:15:57,830 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-09-04 23:15:58,146 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-09-04 23:15:58,208 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-09-04 23:15:58,216 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-09-04 23:15:58,226 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-09-04 23:15:58,231 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-09-04 23:15:58,234 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-09-04 23:15:58,234 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-09-04 23:15:58,235 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-09-04 23:15:58,363 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-09-04 23:15:58,365 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-09-04 23:15:58,398 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-09-04 23:15:58,399 INFO org.mortbay.log: jetty-6.1.26
2016-09-04 23:15:58,616 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-09-04 23:15:58,686 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-09-04 23:15:58,686 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-09-04 23:15:58,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-09-04 23:15:58,721 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-09-04 23:15:58,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-09-04 23:15:58,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-09-04 23:15:58,774 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-09-04 23:15:58,775 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 sept. 04 23:15:58
2016-09-04 23:15:58,779 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-09-04 23:15:58,782 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:15:58,786 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-09-04 23:15:58,786 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-09-04 23:15:58,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-09-04 23:15:58,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-09-04 23:15:58,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-09-04 23:15:58,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-09-04 23:15:58,803 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-09-04 23:15:58,805 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-09-04 23:15:58,908 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-09-04 23:15:58,908 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:15:58,909 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-09-04 23:15:58,909 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-09-04 23:15:58,911 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-09-04 23:15:58,911 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-09-04 23:15:58,912 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-09-04 23:15:58,912 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-09-04 23:15:58,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-09-04 23:15:58,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:15:58,920 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-09-04 23:15:58,920 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-09-04 23:15:58,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-09-04 23:15:58,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-09-04 23:15:58,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-09-04 23:15:58,925 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-09-04 23:15:58,925 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-09-04 23:15:58,925 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-09-04 23:15:58,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-09-04 23:15:58,928 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-09-04 23:15:58,930 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-09-04 23:15:58,930 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:15:58,930 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-09-04 23:15:58,930 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-09-04 23:15:58,937 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-jlejeune/dfs/name does not exist
2016-09-04 23:15:58,940 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
2016-09-04 23:15:58,965 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-09-04 23:15:59,066 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2016-09-04 23:15:59,067 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2016-09-04 23:15:59,068 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2016-09-04 23:15:59,071 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
2016-09-04 23:15:59,075 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2016-09-04 23:15:59,077 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-09-04 23:17:24,114 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-09-04 23:17:24,124 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-09-04 23:17:24,132 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-09-04 23:17:24,635 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-09-04 23:17:24,795 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-09-04 23:17:24,795 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-09-04 23:17:24,802 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-09-04 23:17:24,804 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-09-04 23:17:25,237 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-09-04 23:17:25,323 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-09-04 23:17:25,341 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-09-04 23:17:25,349 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-09-04 23:17:25,361 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-09-04 23:17:25,363 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-09-04 23:17:25,363 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-09-04 23:17:25,363 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-09-04 23:17:25,562 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-09-04 23:17:25,564 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-09-04 23:17:25,593 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-09-04 23:17:25,593 INFO org.mortbay.log: jetty-6.1.26
2016-09-04 23:17:25,864 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-09-04 23:17:25,950 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-09-04 23:17:25,950 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-09-04 23:17:25,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-09-04 23:17:25,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-09-04 23:17:26,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-09-04 23:17:26,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-09-04 23:17:26,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-09-04 23:17:26,051 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 sept. 04 23:17:26
2016-09-04 23:17:26,053 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-09-04 23:17:26,054 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:17:26,056 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-09-04 23:17:26,056 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-09-04 23:17:26,078 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-09-04 23:17:26,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-09-04 23:17:26,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-09-04 23:17:26,096 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-09-04 23:17:26,097 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-09-04 23:17:26,099 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-09-04 23:17:26,213 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-09-04 23:17:26,214 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:17:26,214 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-09-04 23:17:26,215 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-09-04 23:17:26,215 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-09-04 23:17:26,215 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-09-04 23:17:26,215 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-09-04 23:17:26,216 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-09-04 23:17:26,226 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-09-04 23:17:26,226 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:17:26,226 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-09-04 23:17:26,226 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-09-04 23:17:26,228 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-09-04 23:17:26,229 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-09-04 23:17:26,229 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-09-04 23:17:26,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-09-04 23:17:26,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-09-04 23:17:26,232 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-09-04 23:17:26,236 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-09-04 23:17:26,236 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-09-04 23:17:26,239 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-09-04 23:17:26,239 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-09-04 23:17:26,239 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-09-04 23:17:26,239 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-09-04 23:17:26,255 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 4402@vm-ubuntu
2016-09-04 23:17:26,383 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-09-04 23:17:26,391 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-09-04 23:17:26,391 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-09-04 23:17:26,448 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-09-04 23:17:26,509 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-09-04 23:17:26,509 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-09-04 23:17:26,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-09-04 23:17:26,527 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-09-04 23:17:26,732 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-09-04 23:17:26,732 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 488 msecs
2016-09-04 23:17:27,067 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-09-04 23:17:27,081 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-09-04 23:17:27,111 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-09-04 23:17:27,168 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-09-04 23:17:27,189 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-09-04 23:17:27,189 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-09-04 23:17:27,189 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-09-04 23:17:27,189 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2016-09-04 23:17:27,190 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-09-04 23:17:27,190 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-09-04 23:17:27,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-09-04 23:17:27,215 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2016-09-04 23:17:27,277 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-09-04 23:17:27,279 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-09-04 23:17:27,283 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-09-04 23:17:27,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-09-04 23:17:27,293 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-09-04 23:17:32,357 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e31dbd5b-d1f3-4ed0-aaa4-93c530427816, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-26d74349-f25f-4e73-bb40-27a362aff91f;nsid=1955297735;c=0) storage e31dbd5b-d1f3-4ed0-aaa4-93c530427816
2016-09-04 23:17:32,357 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-09-04 23:17:32,362 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-09-04 23:17:32,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-09-04 23:17:32,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-636ee46f-3b77-4f91-a5e7-8cbb2714777f for DN 127.0.0.1:50010
2016-09-04 23:17:32,993 INFO BlockStateChange: BLOCK* processReport: from storage DS-636ee46f-3b77-4f91-a5e7-8cbb2714777f node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=e31dbd5b-d1f3-4ed0-aaa4-93c530427816, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-26d74349-f25f-4e73-bb40-27a362aff91f;nsid=1955297735;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2016-09-04 23:18:10,021 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-09-04 23:18:10,024 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-10-13 11:37:19,705 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-10-13 11:37:19,720 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-10-13 11:37:19,726 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-10-13 11:37:20,122 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-10-13 11:37:20,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-10-13 11:37:20,249 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-10-13 11:37:20,251 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-10-13 11:37:20,252 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-10-13 11:37:20,572 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-10-13 11:37:20,708 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-10-13 11:37:20,720 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-10-13 11:37:20,730 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-10-13 11:37:20,736 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-10-13 11:37:20,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-10-13 11:37:20,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-10-13 11:37:20,738 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-10-13 11:37:20,904 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-10-13 11:37:20,907 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-10-13 11:37:20,927 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-10-13 11:37:20,927 INFO org.mortbay.log: jetty-6.1.26
2016-10-13 11:37:21,060 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-10-13 11:37:21,109 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-10-13 11:37:21,109 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-10-13 11:37:21,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-10-13 11:37:21,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-10-13 11:37:21,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-10-13 11:37:21,192 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-10-13 11:37:21,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-10-13 11:37:21,194 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 oct. 13 11:37:21
2016-10-13 11:37:21,196 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-10-13 11:37:21,196 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:37:21,197 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-10-13 11:37:21,197 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-10-13 11:37:21,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-10-13 11:37:21,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-10-13 11:37:21,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-10-13 11:37:21,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-10-13 11:37:21,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-10-13 11:37:21,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-10-13 11:37:21,272 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-10-13 11:37:21,272 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:37:21,273 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-10-13 11:37:21,273 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-10-13 11:37:21,273 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-10-13 11:37:21,274 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-10-13 11:37:21,274 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-10-13 11:37:21,274 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-10-13 11:37:21,282 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-10-13 11:37:21,283 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:37:21,283 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-10-13 11:37:21,283 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-10-13 11:37:21,284 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-10-13 11:37:21,284 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-10-13 11:37:21,284 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-10-13 11:37:21,288 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-10-13 11:37:21,288 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-10-13 11:37:21,288 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-10-13 11:37:21,289 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-10-13 11:37:21,289 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-10-13 11:37:21,292 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-10-13 11:37:21,292 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:37:21,292 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-10-13 11:37:21,292 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-10-13 11:37:21,296 WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-jlejeune/dfs/name does not exist
2016-10-13 11:37:21,298 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage
2016-10-13 11:37:21,308 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-10-13 11:37:21,431 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2016-10-13 11:37:21,432 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2016-10-13 11:37:21,432 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2016-10-13 11:37:21,433 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
2016-10-13 11:37:21,436 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2016-10-13 11:37:21,438 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-10-13 11:37:55,315 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-10-13 11:37:55,323 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-10-13 11:37:55,328 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-10-13 11:37:55,624 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-10-13 11:37:55,703 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-10-13 11:37:55,703 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-10-13 11:37:55,706 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-10-13 11:37:55,707 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-10-13 11:37:55,931 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-10-13 11:37:55,991 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-10-13 11:37:55,998 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-10-13 11:37:56,004 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-10-13 11:37:56,009 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-10-13 11:37:56,011 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-10-13 11:37:56,011 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-10-13 11:37:56,011 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-10-13 11:37:56,130 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-10-13 11:37:56,131 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-10-13 11:37:56,144 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-10-13 11:37:56,144 INFO org.mortbay.log: jetty-6.1.26
2016-10-13 11:37:56,276 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-10-13 11:38:01,324 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-10-13 11:38:01,324 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-10-13 11:38:01,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-10-13 11:38:01,354 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-10-13 11:38:01,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-10-13 11:38:01,390 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-10-13 11:38:01,391 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-10-13 11:38:01,392 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 oct. 13 11:38:01
2016-10-13 11:38:01,393 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-10-13 11:38:01,393 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:38:01,395 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-10-13 11:38:01,395 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-10-13 11:38:01,404 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-10-13 11:38:01,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-10-13 11:38:01,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-10-13 11:38:01,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-10-13 11:38:01,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-10-13 11:38:01,413 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-10-13 11:38:01,481 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-10-13 11:38:01,481 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:38:01,482 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-10-13 11:38:01,482 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-10-13 11:38:01,484 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-10-13 11:38:01,484 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-10-13 11:38:01,484 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-10-13 11:38:01,484 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-10-13 11:38:01,492 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-10-13 11:38:01,492 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:38:01,493 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-10-13 11:38:01,493 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-10-13 11:38:01,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-10-13 11:38:01,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-10-13 11:38:01,495 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-10-13 11:38:01,498 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-10-13 11:38:01,498 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-10-13 11:38:01,498 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-10-13 11:38:01,500 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-10-13 11:38:01,500 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-10-13 11:38:01,503 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-10-13 11:38:01,503 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-10-13 11:38:01,503 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-10-13 11:38:01,503 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-10-13 11:38:01,516 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 3755@vm-ubuntu
2016-10-13 11:38:01,580 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-10-13 11:38:01,581 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-10-13 11:38:01,581 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-10-13 11:38:01,619 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-10-13 11:38:01,658 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-10-13 11:38:01,658 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-10-13 11:38:01,665 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-10-13 11:38:01,666 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-10-13 11:38:01,782 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-10-13 11:38:01,782 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 275 msecs
2016-10-13 11:38:02,091 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-10-13 11:38:02,097 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-10-13 11:38:02,109 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-10-13 11:38:02,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-10-13 11:38:02,149 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-10-13 11:38:02,149 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-10-13 11:38:02,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-10-13 11:38:02,149 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2016-10-13 11:38:02,154 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-10-13 11:38:02,154 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-10-13 11:38:02,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-10-13 11:38:02,167 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-10-13 11:38:02,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-10-13 11:38:02,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-10-13 11:38:02,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-10-13 11:38:02,168 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-10-13 11:38:02,168 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2016-10-13 11:38:02,207 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-10-13 11:38:02,224 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-10-13 11:38:02,224 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-10-13 11:38:02,228 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-10-13 11:38:02,238 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-10-13 11:38:02,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b266d49a-f75e-495b-b30c-cf23a9ce11c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c811ca90-0eee-44f2-b509-cffb705c50db;nsid=1179288442;c=0) storage b266d49a-f75e-495b-b30c-cf23a9ce11c5
2016-10-13 11:38:02,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-10-13 11:38:02,775 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-10-13 11:38:02,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-10-13 11:38:02,851 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-29f3f2a5-d104-4f26-b730-ea30b4ef3b2c for DN 127.0.0.1:50010
2016-10-13 11:38:02,900 INFO BlockStateChange: BLOCK* processReport: from storage DS-29f3f2a5-d104-4f26-b730-ea30b4ef3b2c node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b266d49a-f75e-495b-b30c-cf23a9ce11c5, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c811ca90-0eee-44f2-b509-cffb705c50db;nsid=1179288442;c=0), blocks: 0, hasStaleStorage: false, processing time: 3 msecs
2016-10-13 11:39:28,641 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2016-10-13 11:52:24,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 11 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 0 Number of syncs: 10 SyncTimes(ms): 18 
2016-10-13 11:58:20,814 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 0 Number of syncs: 19 SyncTimes(ms): 32 
2016-10-13 12:02:18,270 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 23 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 22 SyncTimes(ms): 34 
2016-10-13 12:04:13,245 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 29 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 28 SyncTimes(ms): 45 
2016-10-13 12:05:26,887 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 32 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 31 SyncTimes(ms): 53 
2016-10-13 12:07:07,046 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-10-13 12:07:07,047 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-10-13 12:07:07,047 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2016-10-13 12:07:07,049 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 38 Total time for transactions(ms): 23 Number of transactions batched in Syncs: 0 Number of syncs: 37 SyncTimes(ms): 63 
2016-10-13 12:07:07,051 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 38 Total time for transactions(ms): 23 Number of transactions batched in Syncs: 0 Number of syncs: 38 SyncTimes(ms): 65 
2016-10-13 12:07:07,057 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000038
2016-10-13 12:07:07,063 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 39
2016-10-13 12:07:08,220 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 166,67 KB/s
2016-10-13 12:07:08,220 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000038 size 1048 bytes.
2016-10-13 12:07:08,229 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2016-10-13 12:11:13,304 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 6 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 14 
2016-10-13 12:13:18,610 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 7 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 4 SyncTimes(ms): 15 
2016-10-13 12:14:13,491 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /home/jlejeune/spark-warehouse/toto/.hive-staging_hive_2016-10-13_12-14-12_849_6890612558265792428-1/-ext-10000/_temporary/0/_temporary/attempt_201610131214_0003_m_000000_0/part-00000 is closed by DFSClient_NONMAPREDUCE_-500925131_1
2016-10-13 12:14:13,544 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-29f3f2a5-d104-4f26-b730-ea30b4ef3b2c:NORMAL:127.0.0.1:50010|RBW]]} for /home/jlejeune/spark-warehouse/toto/.hive-staging_hive_2016-10-13_12-14-12_849_6890612558265792428-1/-ext-10000/_temporary/0/_temporary/attempt_201610131214_0003_m_000001_0/part-00001
2016-10-13 12:14:13,932 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-29f3f2a5-d104-4f26-b730-ea30b4ef3b2c:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /home/jlejeune/spark-warehouse/toto/.hive-staging_hive_2016-10-13_12-14-12_849_6890612558265792428-1/-ext-10000/_temporary/0/_temporary/attempt_201610131214_0003_m_000001_0/part-00001
2016-10-13 12:14:13,981 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-29f3f2a5-d104-4f26-b730-ea30b4ef3b2c:NORMAL:127.0.0.1:50010|RBW]]} size 3
2016-10-13 12:14:14,340 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /home/jlejeune/spark-warehouse/toto/.hive-staging_hive_2016-10-13_12-14-12_849_6890612558265792428-1/-ext-10000/_temporary/0/_temporary/attempt_201610131214_0003_m_000001_0/part-00001 is closed by DFSClient_NONMAPREDUCE_-500925131_1
2016-10-13 12:14:14,395 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /home/jlejeune/spark-warehouse/toto/.hive-staging_hive_2016-10-13_12-14-12_849_6890612558265792428-1/-ext-10000/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-500925131_1
2016-10-13 12:23:41,275 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 33 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 2 Number of syncs: 22 SyncTimes(ms): 42 
2016-10-13 12:44:50,510 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-10-13 12:44:50,536 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-11-25 15:28:33,149 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-11-25 15:28:33,163 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-11-25 15:28:33,167 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-11-25 15:28:33,564 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-11-25 15:28:33,708 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-11-25 15:28:33,708 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-11-25 15:28:33,713 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-11-25 15:28:33,713 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-11-25 15:28:33,981 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-11-25 15:28:34,050 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-11-25 15:28:34,058 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-11-25 15:28:34,064 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-11-25 15:28:34,070 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-11-25 15:28:34,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-11-25 15:28:34,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-11-25 15:28:34,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-11-25 15:28:34,223 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-11-25 15:28:34,226 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-11-25 15:28:34,248 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-11-25 15:28:34,248 INFO org.mortbay.log: jetty-6.1.26
2016-11-25 15:28:34,421 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-11-25 15:28:34,450 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-11-25 15:28:34,450 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-11-25 15:28:34,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-11-25 15:28:34,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-11-25 15:28:34,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-11-25 15:28:34,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-11-25 15:28:34,548 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-11-25 15:28:34,549 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 nov. 25 15:28:34
2016-11-25 15:28:34,551 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-11-25 15:28:34,552 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-25 15:28:34,554 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-11-25 15:28:34,554 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-11-25 15:28:34,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-11-25 15:28:34,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-11-25 15:28:34,567 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-11-25 15:28:34,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-11-25 15:28:34,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-11-25 15:28:34,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-11-25 15:28:34,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-11-25 15:28:34,568 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-11-25 15:28:34,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-11-25 15:28:34,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-11-25 15:28:34,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-11-25 15:28:34,576 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-11-25 15:28:34,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-11-25 15:28:34,637 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-11-25 15:28:34,637 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-25 15:28:34,637 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-11-25 15:28:34,637 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-11-25 15:28:34,640 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-11-25 15:28:34,640 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-11-25 15:28:34,640 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-11-25 15:28:34,641 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-11-25 15:28:34,648 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-11-25 15:28:34,648 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-25 15:28:34,648 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-11-25 15:28:34,648 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-11-25 15:28:34,650 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-11-25 15:28:34,650 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-11-25 15:28:34,650 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-11-25 15:28:34,653 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-11-25 15:28:34,653 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-11-25 15:28:34,653 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-11-25 15:28:34,655 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-11-25 15:28:34,655 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-11-25 15:28:34,657 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-11-25 15:28:34,657 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-25 15:28:34,657 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-11-25 15:28:34,657 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-11-25 15:28:34,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 14763@vm-ubuntu
2016-11-25 15:28:34,744 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-11-25 15:28:34,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-11-25 15:28:34,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-11-25 15:28:34,781 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-11-25 15:28:34,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-11-25 15:28:34,807 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-11-25 15:28:34,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-11-25 15:28:34,816 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-11-25 15:28:34,939 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-11-25 15:28:34,939 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 279 msecs
2016-11-25 15:28:35,298 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-11-25 15:28:35,309 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-11-25 15:28:35,319 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-11-25 15:28:35,371 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-11-25 15:28:35,386 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-11-25 15:28:35,387 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-11-25 15:28:35,387 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-11-25 15:28:35,387 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2016-11-25 15:28:35,387 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-11-25 15:28:35,387 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-11-25 15:28:35,397 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-25 15:28:35,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-11-25 15:28:35,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-11-25 15:28:35,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-11-25 15:28:35,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-11-25 15:28:35,403 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-11-25 15:28:35,404 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2016-11-25 15:28:35,442 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-11-25 15:28:35,443 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-11-25 15:28:35,449 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-11-25 15:28:35,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-11-25 15:28:35,461 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-11-25 15:28:39,545 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=970c6438-a8f2-4990-97b2-a5ab81cb07df, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f0591f95-6a9a-4044-bc2f-55b5c26455bf;nsid=1071188204;c=0) storage 970c6438-a8f2-4990-97b2-a5ab81cb07df
2016-11-25 15:28:39,546 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-25 15:28:39,546 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-11-25 15:28:39,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-25 15:28:39,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-d80d0458-a5bb-4c6f-b0ba-5ee27a721dd4 for DN 127.0.0.1:50010
2016-11-25 15:28:39,696 INFO BlockStateChange: BLOCK* processReport: from storage DS-d80d0458-a5bb-4c6f-b0ba-5ee27a721dd4 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=970c6438-a8f2-4990-97b2-a5ab81cb07df, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-f0591f95-6a9a-4044-bc2f-55b5c26455bf;nsid=1071188204;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2016-11-25 15:29:30,364 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d80d0458-a5bb-4c6f-b0ba-5ee27a721dd4:NORMAL:127.0.0.1:50010|RBW]]} for /showme._COPYING_
2016-11-25 15:29:30,756 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d80d0458-a5bb-4c6f-b0ba-5ee27a721dd4:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /showme._COPYING_
2016-11-25 15:29:30,756 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2016-11-25 15:29:30,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-d80d0458-a5bb-4c6f-b0ba-5ee27a721dd4:NORMAL:127.0.0.1:50010|RBW]]} size 243
2016-11-25 15:29:31,190 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /showme._COPYING_ is closed by DFSClient_NONMAPREDUCE_1821987202_1
2016-11-25 15:29:44,025 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-11-25 15:29:44,025 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-11-25 15:29:44,025 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2016-11-25 15:29:44,026 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 26 
2016-11-25 15:29:44,029 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 30 
2016-11-25 15:29:44,032 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000008
2016-11-25 15:29:44,034 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 9
2016-11-25 15:29:45,317 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 0,00 KB/s
2016-11-25 15:29:45,317 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000008 size 441 bytes.
2016-11-25 15:29:45,326 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2016-11-25 16:21:03,179 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-11-25 16:21:03,181 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-11-30 10:58:52,423 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-11-30 10:58:52,440 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-11-30 10:58:52,445 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-11-30 10:58:52,834 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-11-30 10:58:53,020 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-11-30 10:58:53,020 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-11-30 10:58:53,023 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-11-30 10:58:53,024 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-11-30 10:58:53,251 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-11-30 10:58:53,315 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-11-30 10:58:53,326 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-11-30 10:58:53,342 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-11-30 10:58:53,348 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-11-30 10:58:53,351 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-11-30 10:58:53,351 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-11-30 10:58:53,352 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-11-30 10:58:53,530 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-11-30 10:58:53,532 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-11-30 10:58:53,562 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-11-30 10:58:53,562 INFO org.mortbay.log: jetty-6.1.26
2016-11-30 10:58:53,742 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-11-30 10:58:53,772 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-11-30 10:58:53,772 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-11-30 10:58:53,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-11-30 10:58:53,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-11-30 10:58:53,847 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-11-30 10:58:53,847 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-11-30 10:58:53,849 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-11-30 10:58:53,849 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 nov. 30 10:58:53
2016-11-30 10:58:53,851 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-11-30 10:58:53,851 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-30 10:58:53,853 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-11-30 10:58:53,853 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-11-30 10:58:53,861 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-11-30 10:58:53,868 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-11-30 10:58:53,868 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-11-30 10:58:53,868 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-11-30 10:58:53,868 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-11-30 10:58:53,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-11-30 10:58:53,973 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-11-30 10:58:53,973 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-30 10:58:53,974 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-11-30 10:58:53,974 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-11-30 10:58:53,976 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-11-30 10:58:53,976 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-11-30 10:58:53,977 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-11-30 10:58:53,977 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-11-30 10:58:53,985 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-11-30 10:58:53,985 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-30 10:58:53,985 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-11-30 10:58:53,985 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-11-30 10:58:53,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-11-30 10:58:53,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-11-30 10:58:53,988 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-11-30 10:58:53,992 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-11-30 10:58:53,992 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-11-30 10:58:53,993 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-11-30 10:58:53,996 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-11-30 10:58:53,996 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-11-30 10:58:53,997 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-11-30 10:58:53,998 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-11-30 10:58:53,998 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-11-30 10:58:53,998 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-11-30 10:58:54,016 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 3416@vm-ubuntu
2016-11-30 10:58:54,092 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-11-30 10:58:54,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-11-30 10:58:54,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-11-30 10:58:54,131 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-11-30 10:58:54,158 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-11-30 10:58:54,159 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-11-30 10:58:54,167 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-11-30 10:58:54,167 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-11-30 10:58:54,272 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-11-30 10:58:54,272 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 270 msecs
2016-11-30 10:58:54,637 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-11-30 10:58:54,648 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-11-30 10:58:54,660 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-11-30 10:58:54,691 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-11-30 10:58:54,703 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-11-30 10:58:54,703 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-11-30 10:58:54,703 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-11-30 10:58:54,704 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2016-11-30 10:58:54,704 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-11-30 10:58:54,704 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-11-30 10:58:54,717 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-11-30 10:58:54,730 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 26 msec
2016-11-30 10:58:54,769 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-11-30 10:58:54,770 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-11-30 10:58:54,774 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-11-30 10:58:54,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-11-30 10:58:54,782 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-11-30 10:58:58,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=7165b9be-b4f2-4f9c-b8b9-b31647e2c7af, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c71d53ba-4eda-4011-ba6d-d7163526e197;nsid=1400059357;c=0) storage 7165b9be-b4f2-4f9c-b8b9-b31647e2c7af
2016-11-30 10:58:58,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-30 10:58:58,711 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-11-30 10:58:58,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-11-30 10:58:58,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-01367245-2e01-4277-9bb6-bdd13b036f0f for DN 127.0.0.1:50010
2016-11-30 10:58:58,889 INFO BlockStateChange: BLOCK* processReport: from storage DS-01367245-2e01-4277-9bb6-bdd13b036f0f node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=7165b9be-b4f2-4f9c-b8b9-b31647e2c7af, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c71d53ba-4eda-4011-ba6d-d7163526e197;nsid=1400059357;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2016-11-30 10:59:23,240 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.version
2016-11-30 10:59:23,616 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/.tmp/hbase.version
2016-11-30 10:59:23,616 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2016-11-30 10:59:23,638 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 7
2016-11-30 10:59:24,039 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.version is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:24,086 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.id
2016-11-30 10:59:24,099 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 10:59:24,104 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.id is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:24,252 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.regioninfo
2016-11-30 10:59:24,267 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 10:59:24,271 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.regioninfo is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:24,539 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:24,568 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001
2016-11-30 10:59:24,580 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 10:59:24,585 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:25,977 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480499965897
2016-11-30 10:59:26,108 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480499965897 for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 10:59:30,702 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480499970688.meta
2016-11-30 10:59:30,760 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480499970688.meta for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 10:59:31,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/3.seqid is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 10:59:32,091 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000001.log
2016-11-30 10:59:32,115 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000001.log for DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:32,314 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001
2016-11-30 10:59:32,343 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 10:59:32,351 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:32,390 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/.regioninfo
2016-11-30 10:59:32,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 10:59:32,410 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/.regioninfo is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:33,016 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 10:59:55,158 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 84 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 8 Number of syncs: 54 SyncTimes(ms): 119 
2016-11-30 10:59:55,182 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 475
2016-11-30 10:59:55,186 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000001.log is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 10:59:55,192 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2016-11-30 10:59:57,757 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741831_1007]
2016-11-30 11:05:39,086 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 87 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 8 Number of syncs: 57 SyncTimes(ms): 123 
2016-11-30 11:05:39,140 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/b342078b55d04bb2abb4a72e9e9ba358
2016-11-30 11:05:39,157 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 11:05:39,162 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/b342078b55d04bb2abb4a72e9e9ba358 is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:08:31,674 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 94 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 8 Number of syncs: 62 SyncTimes(ms): 138 
2016-11-30 11:08:31,700 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/.tmp/c40b7877fffb4fd9a2e2579d282ab2e7
2016-11-30 11:08:31,712 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 11:08:31,715 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/.tmp/c40b7877fffb4fd9a2e2579d282ab2e7 is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:17:03,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-11-30 11:17:03,580 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-11-30 11:17:03,580 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2016-11-30 11:17:03,580 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 101 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 8 Number of syncs: 67 SyncTimes(ms): 145 
2016-11-30 11:17:03,581 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 101 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 8 Number of syncs: 68 SyncTimes(ms): 147 
2016-11-30 11:17:03,583 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000101
2016-11-30 11:17:03,588 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 102
2016-11-30 11:17:04,724 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1500,00 KB/s
2016-11-30 11:17:04,724 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000101 size 3481 bytes.
2016-11-30 11:17:04,727 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2016-11-30 11:59:26,500 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-11-30 11:59:26,508 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480503566492
2016-11-30 11:59:26,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480503566492 for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:59:26,529 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 11:59:26,532 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480499965897 is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:59:31,104 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480503571091.meta
2016-11-30 11:59:31,111 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480503571091.meta for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:59:31,119 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 11:59:31,121 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480499970688.meta is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 11:59:55,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000002.log
2016-11-30 11:59:55,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 11:59:55,216 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000002.log is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 11:59:55,222 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 127.0.0.1:50010 
2016-11-30 11:59:56,181 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741838_1014]
2016-11-30 12:10:25,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 25 
2016-11-30 12:10:25,215 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2016-11-30 12:10:25,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2016-11-30 12:10:26,481 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005, blk_1073741830_1006]
2016-11-30 12:17:05,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-11-30 12:17:05,487 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-11-30 12:17:05,487 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 102
2016-11-30 12:17:05,487 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 20 SyncTimes(ms): 31 
2016-11-30 12:17:05,491 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 21 SyncTimes(ms): 35 
2016-11-30 12:17:05,493 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000102 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000102-0000000000000000129
2016-11-30 12:17:05,494 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 130
2016-11-30 12:17:05,588 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1500,00 KB/s
2016-11-30 12:17:05,588 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000129 size 3482 bytes.
2016-11-30 12:17:05,592 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 101
2016-11-30 12:17:05,592 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-11-30 12:59:27,087 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2016-11-30 12:59:27,092 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480507167080
2016-11-30 12:59:27,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480507167080 for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 12:59:27,110 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 12:59:27,111 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480503566492 is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 12:59:31,571 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480507171555.meta
2016-11-30 12:59:31,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480507171555.meta for DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 12:59:31,603 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 12:59:31,607 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480503571091.meta is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 12:59:55,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000003.log
2016-11-30 12:59:55,240 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 12:59:55,249 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000003.log is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 12:59:55,252 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741841_1017 127.0.0.1:50010 
2016-11-30 12:59:57,825 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741841_1017]
2016-11-30 13:10:25,267 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 32 
2016-11-30 13:10:25,270 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 127.0.0.1:50010 
2016-11-30 13:10:25,273 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010 
2016-11-30 13:10:28,101 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741836_1012, blk_1073741837_1013]
2016-11-30 13:17:06,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-11-30 13:17:06,302 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-11-30 13:17:06,302 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 130
2016-11-30 13:17:06,304 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 20 SyncTimes(ms): 36 
2016-11-30 13:17:06,311 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 21 SyncTimes(ms): 44 
2016-11-30 13:17:06,317 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000130 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000130-0000000000000000157
2016-11-30 13:17:06,318 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 158
2016-11-30 13:17:06,368 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1000,00 KB/s
2016-11-30 13:17:06,369 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000157 size 3482 bytes.
2016-11-30 13:17:06,372 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 129
2016-11-30 13:17:06,372 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000101, cpktTxId=0000000000000000101)
2016-11-30 13:43:50,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000004.log
2016-11-30 13:43:50,325 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2016-11-30 13:43:50,335 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-11-30 13:43:50,339 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000004.log is closed by DFSClient_NONMAPREDUCE_1841673742_1
2016-11-30 13:43:50,562 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/eff00f09f801bc58543e47041eac0ef0/recovered.edits/10.seqid is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 13:43:56,693 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/11.seqid is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 13:43:56,887 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 13:43:56,891 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192..meta.1480507171555.meta is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 13:43:56,912 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-01367245-2e01-4277-9bb6-bdd13b036f0f:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-11-30 13:43:56,914 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480499960192/vm-ubuntu%2C16201%2C1480499960192.default.1480507167080 is closed by DFSClient_NONMAPREDUCE_1888956638_1
2016-11-30 13:44:16,238 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-11-30 13:44:16,239 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-12-01 11:39:06,481 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-12-01 11:39:06,494 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-12-01 11:39:06,498 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-12-01 11:39:06,924 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-12-01 11:39:07,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-12-01 11:39:07,114 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-12-01 11:39:07,117 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-12-01 11:39:07,121 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-12-01 11:39:07,369 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-12-01 11:39:07,435 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-12-01 11:39:07,443 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-12-01 11:39:07,449 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-12-01 11:39:07,455 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-12-01 11:39:07,458 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-12-01 11:39:07,458 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-12-01 11:39:07,459 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-12-01 11:39:07,706 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-12-01 11:39:07,709 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-12-01 11:39:07,769 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-12-01 11:39:07,770 INFO org.mortbay.log: jetty-6.1.26
2016-12-01 11:39:08,032 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-12-01 11:39:08,081 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-01 11:39:08,081 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-01 11:39:08,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-12-01 11:39:08,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-12-01 11:39:08,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-12-01 11:39:08,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-12-01 11:39:08,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-12-01 11:39:08,231 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 déc. 01 11:39:08
2016-12-01 11:39:08,233 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-12-01 11:39:08,233 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 11:39:08,240 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-12-01 11:39:08,240 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-12-01 11:39:08,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-12-01 11:39:08,252 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-12-01 11:39:08,261 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-12-01 11:39:08,261 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-12-01 11:39:08,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-12-01 11:39:08,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-12-01 11:39:08,266 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-12-01 11:39:08,347 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-12-01 11:39:08,347 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 11:39:08,347 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-12-01 11:39:08,347 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-12-01 11:39:08,348 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-12-01 11:39:08,348 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-12-01 11:39:08,348 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-12-01 11:39:08,349 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-12-01 11:39:08,356 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-12-01 11:39:08,356 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 11:39:08,356 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-12-01 11:39:08,356 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-12-01 11:39:08,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-12-01 11:39:08,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-12-01 11:39:08,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-12-01 11:39:08,366 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-12-01 11:39:08,366 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-12-01 11:39:08,366 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-12-01 11:39:08,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-12-01 11:39:08,377 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-12-01 11:39:08,379 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-12-01 11:39:08,379 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 11:39:08,380 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-12-01 11:39:08,380 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-12-01 11:39:08,409 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 3275@vm-ubuntu
2016-12-01 11:39:08,545 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-12-01 11:39:08,546 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-12-01 11:39:08,546 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-12-01 11:39:08,607 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-12-01 11:39:08,640 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-12-01 11:39:08,640 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-12-01 11:39:08,652 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-12-01 11:39:08,653 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-12-01 11:39:08,764 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-12-01 11:39:08,764 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 380 msecs
2016-12-01 11:39:09,120 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-12-01 11:39:09,129 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-12-01 11:39:09,142 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-12-01 11:39:09,170 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-12-01 11:39:09,180 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-12-01 11:39:09,181 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-12-01 11:39:09,181 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-12-01 11:39:09,181 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2016-12-01 11:39:09,181 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-12-01 11:39:09,181 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-12-01 11:39:09,197 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-12-01 11:39:09,205 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2016-12-01 11:39:09,239 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-12-01 11:39:09,240 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-12-01 11:39:09,242 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-12-01 11:39:09,242 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-12-01 11:39:09,251 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-12-01 11:39:12,602 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-793d393b-7cf0-49cf-bc01-fc677d42c2a2;nsid=124849798;c=0) storage b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d
2016-12-01 11:39:12,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 11:39:12,603 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-12-01 11:39:12,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 11:39:12,737 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1 for DN 127.0.0.1:50010
2016-12-01 11:39:12,797 INFO BlockStateChange: BLOCK* processReport: from storage DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-793d393b-7cf0-49cf-bc01-fc677d42c2a2;nsid=124849798;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2016-12-01 11:39:40,405 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.version
2016-12-01 11:39:40,682 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/.tmp/hbase.version
2016-12-01 11:39:40,682 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2016-12-01 11:39:40,709 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 7
2016-12-01 11:39:41,102 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.version is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:41,175 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.id
2016-12-01 11:39:41,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:39:41,209 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.id is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:41,371 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.regioninfo
2016-12-01 11:39:41,383 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:39:41,386 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.regioninfo is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:41,550 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:41,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001
2016-12-01 11:39:41,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:39:41,596 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:42,690 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480588782635
2016-12-01 11:39:42,833 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480588782635 for DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 11:39:47,652 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480588787627.meta
2016-12-01 11:39:47,675 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480588787627.meta for DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 11:39:48,171 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/3.seqid is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 11:39:48,550 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000001.log
2016-12-01 11:39:48,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000001.log for DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:48,812 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001
2016-12-01 11:39:48,842 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:39:48,848 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:48,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/.regioninfo
2016-12-01 11:39:48,887 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:39:48,890 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/.regioninfo is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:39:49,440 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 11:40:12,124 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 84 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 8 Number of syncs: 54 SyncTimes(ms): 64 
2016-12-01 11:40:12,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 475
2016-12-01 11:40:12,139 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000001.log is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 11:40:12,145 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2016-12-01 11:40:12,220 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741831_1007]
2016-12-01 11:40:17,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 11:40:17,432 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 11:40:17,432 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2016-12-01 11:40:17,433 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 87 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 8 Number of syncs: 58 SyncTimes(ms): 67 
2016-12-01 11:40:17,435 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000087
2016-12-01 11:40:17,438 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 88
2016-12-01 11:40:19,393 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1000,00 KB/s
2016-12-01 11:40:19,395 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000087 size 3170 bytes.
2016-12-01 11:40:19,401 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2016-12-01 11:47:15,580 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-12-01 11:47:15,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/.tmp/b3008c1e9dde4f6485a8642b1f50dfbd
2016-12-01 11:47:15,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:47:15,705 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/.tmp/b3008c1e9dde4f6485a8642b1f50dfbd is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 11:47:33,245 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/787311d0682e4f028a4dc176bc8cf701
2016-12-01 11:47:33,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 11:47:33,269 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/787311d0682e4f028a4dc176bc8cf701 is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:39:43,276 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 16 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 12 SyncTimes(ms): 20 
2016-12-01 12:39:43,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480592383267
2016-12-01 12:39:43,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480592383267 for DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:39:43,303 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 12:39:43,306 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480588782635 is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:39:47,953 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480592387941.meta
2016-12-01 12:39:47,961 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480592387941.meta for DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:39:47,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 12:39:47,976 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480588787627.meta is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:40:12,150 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000002.log
2016-12-01 12:40:12,165 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:40:12,169 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000002.log is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 12:40:12,176 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 127.0.0.1:50010 
2016-12-01 12:40:13,226 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741838_1014]
2016-12-01 12:40:20,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 12:40:20,182 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 12:40:20,182 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 88
2016-12-01 12:40:20,184 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 38 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 29 SyncTimes(ms): 41 
2016-12-01 12:40:20,185 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000088 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000088-0000000000000000125
2016-12-01 12:40:20,185 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 126
2016-12-01 12:40:20,319 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1500,00 KB/s
2016-12-01 12:40:20,319 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000125 size 3720 bytes.
2016-12-01 12:40:20,324 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 87
2016-12-01 12:40:20,324 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-12-01 12:42:57,122 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000003.log
2016-12-01 12:42:57,122 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 4 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-12-01 12:42:57,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:42:57,137 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000003.log is closed by DFSClient_NONMAPREDUCE_-946098589_1
2016-12-01 12:42:59,020 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/recovered.edits/10.seqid is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:43:05,038 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/11.seqid is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:43:05,229 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 12:43:05,236 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339..meta.1480592387941.meta is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:43:05,256 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 12:43:05,261 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480588777339/vm-ubuntu%2C16201%2C1480588777339.default.1480592383267 is closed by DFSClient_NONMAPREDUCE_-734941548_1
2016-12-01 12:44:52,461 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 21 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 16 SyncTimes(ms): 30 
2016-12-01 12:44:52,549 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/.tmp/.tableinfo.0000000002
2016-12-01 12:44:52,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:44:52,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/.tmp/.tableinfo.0000000002 is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 12:44:52,787 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 127.0.0.1:50010 
2016-12-01 12:44:55,110 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480592695045
2016-12-01 12:44:55,241 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480592695045 for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 12:44:55,307 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741828_1004]
2016-12-01 12:44:58,309 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480592698291.meta
2016-12-01 12:44:58,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480592698291.meta for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 12:44:59,401 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/12.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 12:45:00,009 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/recovered.edits/11.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 12:49:53,718 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 55 Total time for transactions(ms): 8 Number of transactions batched in Syncs: 4 Number of syncs: 41 SyncTimes(ms): 58 
2016-12-01 12:49:53,722 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2016-12-01 12:49:53,724 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2016-12-01 12:49:55,419 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005, blk_1073741830_1006]
2016-12-01 12:52:25,140 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 57 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 4 Number of syncs: 43 SyncTimes(ms): 61 
2016-12-01 12:52:25,171 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/181a22c6c41d4b27a7f5c0e8098ae8aa
2016-12-01 12:52:25,185 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:52:25,191 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/181a22c6c41d4b27a7f5c0e8098ae8aa is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 12:53:53,469 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 64 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 4 Number of syncs: 48 SyncTimes(ms): 75 
2016-12-01 12:53:53,471 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 127.0.0.1:50010 
2016-12-01 12:53:53,473 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 127.0.0.1:50010 
2016-12-01 12:53:55,484 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741836_1012, blk_1073741837_1013]
2016-12-01 12:55:40,053 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000004.log
2016-12-01 12:55:40,054 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 68 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 4 Number of syncs: 50 SyncTimes(ms): 78 
2016-12-01 12:55:40,094 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000004.log for DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 12:55:40,117 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741839_1015 127.0.0.1:50010 
2016-12-01 12:55:40,289 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connection/.tmp/.tableinfo.0000000001
2016-12-01 12:55:40,299 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:55:40,302 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connection/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 12:55:40,321 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connection/41499f9f35f5e169cc221fb0148c89c7/.regioninfo
2016-12-01 12:55:40,328 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 12:55:40,331 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connection/41499f9f35f5e169cc221fb0148c89c7/.regioninfo is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 12:55:40,521 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741839_1015]
2016-12-01 12:55:40,736 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connection/41499f9f35f5e169cc221fb0148c89c7/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:00:53,550 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 93 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 7 Number of syncs: 66 SyncTimes(ms): 102 
2016-12-01 13:00:53,567 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 477
2016-12-01 13:00:53,578 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000004.log is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 13:00:53,591 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 127.0.0.1:50010 
2016-12-01 13:00:55,637 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741844_1020]
2016-12-01 13:04:51,897 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 96 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 7 Number of syncs: 69 SyncTimes(ms): 105 
2016-12-01 13:04:51,920 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/3e22c04c86fc4e99972936c65ad3e982
2016-12-01 13:04:51,943 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 13:04:51,949 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/3e22c04c86fc4e99972936c65ad3e982 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:04:52,154 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/7d32d6789d8e47cb8fe2b7d461d4a669
2016-12-01 13:04:52,174 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 13:04:52,176 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/7d32d6789d8e47cb8fe2b7d461d4a669 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:09:53,496 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 120 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 7 Number of syncs: 84 SyncTimes(ms): 120 
2016-12-01 13:09:53,498 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 127.0.0.1:50010 
2016-12-01 13:09:53,501 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 127.0.0.1:50010 
2016-12-01 13:09:53,508 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 127.0.0.1:50010 
2016-12-01 13:09:55,803 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741843_1019, blk_1073741847_1023, blk_1073741835_1011]
2016-12-01 13:40:21,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 13:40:21,129 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 13:40:21,129 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 126
2016-12-01 13:40:21,130 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 128 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 7 Number of syncs: 92 SyncTimes(ms): 136 
2016-12-01 13:40:21,133 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 128 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 7 Number of syncs: 93 SyncTimes(ms): 138 
2016-12-01 13:40:21,143 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000126 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000126-0000000000000000253
2016-12-01 13:40:21,144 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 254
2016-12-01 13:40:21,503 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 2000,00 KB/s
2016-12-01 13:40:21,503 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000253 size 4114 bytes.
2016-12-01 13:40:21,512 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 125
2016-12-01 13:40:21,513 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000087, cpktTxId=0000000000000000087)
2016-12-01 13:44:55,727 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 11 
2016-12-01 13:44:55,741 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480596295708
2016-12-01 13:44:55,766 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480596295708 for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:44:55,782 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741841_1017{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480592695045
2016-12-01 13:44:55,786 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 658
2016-12-01 13:44:56,191 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480592695045 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:44:58,638 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480596298627.meta
2016-12-01 13:44:58,646 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480596298627.meta for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:44:58,657 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 13:44:58,663 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480592698291.meta is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 13:55:53,594 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 18 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 1 Number of syncs: 14 SyncTimes(ms): 25 
2016-12-01 13:55:53,597 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741842_1018 127.0.0.1:50010 
2016-12-01 13:55:53,599 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741841_1017 127.0.0.1:50010 
2016-12-01 13:55:53,662 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741841_1017, blk_1073741842_1018]
2016-12-01 14:00:53,592 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 20 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 1 Number of syncs: 16 SyncTimes(ms): 28 
2016-12-01 14:00:53,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000005.log
2016-12-01 14:00:53,633 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 14:00:53,639 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000005.log is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 14:00:53,646 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741851_1027 127.0.0.1:50010 
2016-12-01 14:00:53,763 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741851_1027]
2016-12-01 14:40:22,513 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 14:40:22,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 14:40:22,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 254
2016-12-01 14:40:22,515 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 1 Number of syncs: 20 SyncTimes(ms): 34 
2016-12-01 14:40:22,516 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 1 Number of syncs: 21 SyncTimes(ms): 35 
2016-12-01 14:40:22,518 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000254 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000254-0000000000000000279
2016-12-01 14:40:22,520 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 280
2016-12-01 14:40:22,732 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 2000,00 KB/s
2016-12-01 14:40:22,732 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000279 size 4114 bytes.
2016-12-01 14:40:22,741 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 253
2016-12-01 14:40:22,742 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000125, cpktTxId=0000000000000000125)
2016-12-01 14:44:56,542 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2016-12-01 14:44:56,550 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480599896525
2016-12-01 14:44:56,561 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480599896525 for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 14:44:56,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741849_1025{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480596295708
2016-12-01 14:44:56,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 91
2016-12-01 14:44:56,982 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480596295708 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 14:44:59,033 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480599899021.meta
2016-12-01 14:44:59,041 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480599899021.meta for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 14:44:59,054 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 14:44:59,056 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480596298627.meta is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 14:55:53,508 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 18 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 1 Number of syncs: 14 SyncTimes(ms): 23 
2016-12-01 14:55:53,512 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741850_1026 127.0.0.1:50010 
2016-12-01 14:55:53,517 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741849_1025 127.0.0.1:50010 
2016-12-01 14:55:54,807 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741849_1025, blk_1073741850_1026]
2016-12-01 15:00:53,650 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 22 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 1 Number of syncs: 16 SyncTimes(ms): 26 
2016-12-01 15:00:53,659 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000006.log
2016-12-01 15:00:53,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/MasterProcWALs/state-00000000000000000006.log
2016-12-01 15:00:53,678 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 30
2016-12-01 15:00:54,080 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000006.log is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:00:54,089 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 127.0.0.1:50010 
2016-12-01 15:00:54,880 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741854_1030]
2016-12-01 15:08:16,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000007.log
2016-12-01 15:08:16,661 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 30 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 2 Number of syncs: 20 SyncTimes(ms): 31 
2016-12-01 15:08:16,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000007.log for DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:08:16,873 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001
2016-12-01 15:08:16,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:08:16,904 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:08:16,942 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/.regioninfo
2016-12-01 15:08:16,956 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:08:16,961 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/.regioninfo is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:08:17,447 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:08:53,013 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connection/41499f9f35f5e169cc221fb0148c89c7/recovered.edits/4.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:08:56,827 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 127.0.0.1:50010 
2016-12-01 15:08:56,834 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 127.0.0.1:50010 
2016-12-01 15:08:57,992 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741845_1021, blk_1073741846_1022]
2016-12-01 15:09:35,199 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 65 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 5 Number of syncs: 45 SyncTimes(ms): 56 
2016-12-01 15:09:35,228 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/.tmp/7ad6ff8ef48f46f3800acb2a9d66614a
2016-12-01 15:09:35,247 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:09:35,250 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/.tmp/7ad6ff8ef48f46f3800acb2a9d66614a is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:09:35,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/2ee0ff2d8a41cd7111db439c08221158/recovered.edits/838.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:09:39,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 127.0.0.1:50010 
2016-12-01 15:09:39,401 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 127.0.0.1:50010 
2016-12-01 15:09:40,000 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741856_1032, blk_1073741857_1033]
2016-12-01 15:09:50,600 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001
2016-12-01 15:09:50,614 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:09:50,617 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:09:50,645 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/.regioninfo
2016-12-01 15:09:50,658 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:09:50,662 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/.regioninfo is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:09:50,986 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:12:25,683 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 106 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 8 Number of syncs: 76 SyncTimes(ms): 81 
2016-12-01 15:12:25,720 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/.tmp/9925cf2f1472471480db0223a0d3fda5
2016-12-01 15:12:25,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:12:25,779 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/.tmp/9925cf2f1472471480db0223a0d3fda5 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:12:25,859 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/7d10cc39ce9fdf5e438876ec779e90bd/recovered.edits/2155.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:12:30,580 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 127.0.0.1:50010 
2016-12-01 15:12:30,582 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 127.0.0.1:50010 
2016-12-01 15:12:31,040 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741859_1035, blk_1073741860_1036]
2016-12-01 15:12:37,736 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001
2016-12-01 15:12:37,748 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:12:37,751 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:12:37,768 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/.regioninfo
2016-12-01 15:12:37,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:12:37,781 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/.regioninfo is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:12:38,068 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:14:53,654 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 146 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 11 Number of syncs: 107 SyncTimes(ms): 106 
2016-12-01 15:14:53,683 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 127.0.0.1:50010 
2016-12-01 15:14:55,072 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741858_1034]
2016-12-01 15:17:53,487 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 155 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 11 Number of syncs: 116 SyncTimes(ms): 114 
2016-12-01 15:17:53,488 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741861_1037 127.0.0.1:50010 
2016-12-01 15:17:54,845 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 478
2016-12-01 15:17:54,847 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000007.log is closed by DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 15:17:54,852 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741855_1031 127.0.0.1:50010 
2016-12-01 15:17:55,105 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741861_1037, blk_1073741855_1031]
2016-12-01 15:18:01,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/98bf45486e9647978333d7935cacc297
2016-12-01 15:18:01,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 15:18:01,141 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/98bf45486e9647978333d7935cacc297 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:40:23,455 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 15:40:23,468 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 15:40:23,468 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 280
2016-12-01 15:40:23,473 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 172 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 11 Number of syncs: 131 SyncTimes(ms): 130 
2016-12-01 15:40:23,479 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 172 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 11 Number of syncs: 132 SyncTimes(ms): 135 
2016-12-01 15:40:23,539 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000280 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000280-0000000000000000451
2016-12-01 15:40:23,547 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 452
2016-12-01 15:40:24,237 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 800,00 KB/s
2016-12-01 15:40:24,237 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000451 size 4207 bytes.
2016-12-01 15:40:24,260 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 279
2016-12-01 15:40:24,262 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000253, cpktTxId=0000000000000000253)
2016-12-01 15:44:57,469 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 64 
2016-12-01 15:44:57,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387
2016-12-01 15:44:57,673 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387 for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:44:57,747 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741852_1028{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480599896525
2016-12-01 15:44:57,760 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 579252
2016-12-01 15:44:58,152 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582.default.1480599896525 is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:44:59,425 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta
2016-12-01 15:44:59,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta for DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:44:59,472 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 15:44:59,481 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480592691582/vm-ubuntu%2C16201%2C1480592691582..meta.1480599899021.meta is closed by DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 15:55:53,465 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 16 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 1 Number of syncs: 12 SyncTimes(ms): 75 
2016-12-01 15:55:53,485 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741853_1029 127.0.0.1:50010 
2016-12-01 15:55:55,879 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741853_1029]
2016-12-01 16:06:05,486 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 17 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 1 Number of syncs: 13 SyncTimes(ms): 80 
2016-12-01 16:06:05,620 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /Booble_0._COPYING_
2016-12-01 16:06:06,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:06:06,111 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /Booble_0._COPYING_ is closed by DFSClient_NONMAPREDUCE_739234397_1
2016-12-01 16:10:26,126 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 28 Number of transactions batched in Syncs: 1 Number of syncs: 17 SyncTimes(ms): 96 
2016-12-01 16:10:26,575 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.jar
2016-12-01 16:10:28,210 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:10:28,221 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.jar is closed by DFSClient_NONMAPREDUCE_1786960479_1
2016-12-01 16:10:28,235 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.jar
2016-12-01 16:10:28,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.split
2016-12-01 16:10:28,381 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.split
2016-12-01 16:10:28,407 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:10:28,412 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.split is closed by DFSClient_NONMAPREDUCE_1786960479_1
2016-12-01 16:10:28,448 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.splitmetainfo
2016-12-01 16:10:28,464 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:10:28,467 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1786960479_1
2016-12-01 16:10:28,762 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.xml
2016-12-01 16:10:28,800 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:10:28,804 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job.xml is closed by DFSClient_NONMAPREDUCE_1786960479_1
2016-12-01 16:10:59,669 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job_1480604743198_0001_1_conf.xml
2016-12-01 16:11:00,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741872_1048{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job_1480604743198_0001_1_conf.xml
2016-12-01 16:11:00,348 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 140834
2016-12-01 16:11:00,731 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job_1480604743198_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-1573454821_1
2016-12-01 16:11:32,115 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-12-01 16:11:32,226 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-12-01 16:12:14,503 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-12-01 16:12:14,515 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-12-01 16:12:14,519 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-12-01 16:12:14,947 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-12-01 16:12:15,084 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-12-01 16:12:15,084 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-12-01 16:12:15,086 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-12-01 16:12:15,087 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-12-01 16:12:15,504 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-12-01 16:12:15,642 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-12-01 16:12:15,666 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-12-01 16:12:15,681 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-12-01 16:12:15,693 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-12-01 16:12:15,696 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-12-01 16:12:15,696 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-12-01 16:12:15,696 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-12-01 16:12:15,929 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-12-01 16:12:15,938 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-12-01 16:12:15,966 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-12-01 16:12:15,966 INFO org.mortbay.log: jetty-6.1.26
2016-12-01 16:12:16,162 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-12-01 16:12:16,197 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-01 16:12:16,197 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-01 16:12:16,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-12-01 16:12:16,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-12-01 16:12:16,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-12-01 16:12:16,297 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-12-01 16:12:16,298 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-12-01 16:12:16,299 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 déc. 01 16:12:16
2016-12-01 16:12:16,301 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-12-01 16:12:16,301 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 16:12:16,302 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-12-01 16:12:16,303 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-12-01 16:12:16,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-12-01 16:12:16,313 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-12-01 16:12:16,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-12-01 16:12:16,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-12-01 16:12:16,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-12-01 16:12:16,320 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-12-01 16:12:16,322 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-12-01 16:12:16,402 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-12-01 16:12:16,402 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 16:12:16,403 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-12-01 16:12:16,403 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-12-01 16:12:16,404 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-12-01 16:12:16,404 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-12-01 16:12:16,404 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-12-01 16:12:16,405 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-12-01 16:12:16,416 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-12-01 16:12:16,416 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 16:12:16,416 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-12-01 16:12:16,416 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-12-01 16:12:16,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-12-01 16:12:16,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-12-01 16:12:16,417 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-12-01 16:12:16,421 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-12-01 16:12:16,421 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-12-01 16:12:16,421 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-12-01 16:12:16,424 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-12-01 16:12:16,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-12-01 16:12:16,427 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-12-01 16:12:16,427 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-01 16:12:16,427 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-12-01 16:12:16,427 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-12-01 16:12:16,447 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 20787@vm-ubuntu
2016-12-01 16:12:16,542 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-12-01 16:12:16,677 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000452 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000452-0000000000000000517
2016-12-01 16:12:16,686 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000451, cpktTxId=0000000000000000451)
2016-12-01 16:12:16,723 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 49 INodes.
2016-12-01 16:12:16,826 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-12-01 16:12:16,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 451 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000451
2016-12-01 16:12:16,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@774698ab expecting start txid #452
2016-12-01 16:12:16,827 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000452-0000000000000000517
2016-12-01 16:12:16,828 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000452-0000000000000000517' to transaction ID 452
2016-12-01 16:12:16,871 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000452-0000000000000000517 of size 1048576 edits # 66 loaded in 0 seconds
2016-12-01 16:12:16,872 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-12-01 16:12:16,874 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 518
2016-12-01 16:12:16,990 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-12-01 16:12:16,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 558 msecs
2016-12-01 16:12:17,236 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-12-01 16:12:17,248 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-12-01 16:12:17,262 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-12-01 16:12:17,316 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-12-01 16:12:17,329 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 2
2016-12-01 16:12:17,329 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 2
2016-12-01 16:12:17,330 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
2016-12-01 16:12:17,337 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 16:12:17,384 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-12-01 16:12:17,384 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-12-01 16:12:17,406 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-12-01 16:12:17,406 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-12-01 16:12:17,418 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-12-01 16:12:21,456 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-793d393b-7cf0-49cf-bc01-fc677d42c2a2;nsid=124849798;c=0) storage b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d
2016-12-01 16:12:21,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 16:12:21,459 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-12-01 16:12:21,555 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-01 16:12:21,555 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1 for DN 127.0.0.1:50010
2016-12-01 16:12:21,625 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
2016-12-01 16:12:21,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-12-01 16:12:21,626 INFO BlockStateChange: BLOCK* processReport: from storage DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=b1fe17c9-ae0f-4f54-9e6d-d51162f9a32d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-793d393b-7cf0-49cf-bc01-fc677d42c2a2;nsid=124849798;c=0), blocks: 20, hasStaleStorage: false, processing time: 11 msecs
2016-12-01 16:12:21,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 20
2016-12-01 16:12:21,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-12-01 16:12:21,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 2
2016-12-01 16:12:21,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-12-01 16:12:21,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 2
2016-12-01 16:12:21,643 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2016-12-01 16:12:41,646 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
2016-12-01 16:12:51,656 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2016-12-01 16:12:51,656 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2016-12-01 16:12:51,656 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2016-12-01 16:12:51,656 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2016-12-01 16:13:26,663 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 16:13:26,663 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 16:13:26,663 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 518
2016-12-01 16:13:26,663 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2016-12-01 16:13:26,669 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 15 
2016-12-01 16:13:26,671 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000518 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000518-0000000000000000519
2016-12-01 16:13:26,671 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 520
2016-12-01 16:13:28,196 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 833,33 KB/s
2016-12-01 16:13:28,196 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000519 size 5613 bytes.
2016-12-01 16:13:28,201 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 451
2016-12-01 16:13:28,201 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000279, cpktTxId=0000000000000000279)
2016-12-01 16:25:26,246 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 5 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-12-01 16:25:27,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_-1527218876_1, pendingcreates: 1], src=/hbase/MasterProcWALs/state-00000000000000000008.log from client DFSClient_NONMAPREDUCE_-1527218876_1
2016-12-01 16:25:27,258 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-1527218876_1, pendingcreates: 1], src=/hbase/MasterProcWALs/state-00000000000000000008.log
2016-12-01 16:25:27,259 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.
2016-12-01 16:25:27,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747.default.1480605927878
2016-12-01 16:25:28,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747.default.1480605927878 for DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:32,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_-418518612_1, pendingcreates: 2], src=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta from client DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 16:25:32,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-418518612_1, pendingcreates: 2], src=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta
2016-12-01 16:25:32,125 INFO BlockStateChange: BLOCK* blk_1073741866_1042{UCState=UNDER_RECOVERY, truncateBlock=null, primaryNodeIndex=0, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]]} recovery started, primary=ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]
2016-12-01 16:25:32,125 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseLease: File /hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta has not been closed. Lease recovery is in progress. RecoveryId = 1050 for block blk_1073741866_1042{UCState=UNDER_RECOVERY, truncateBlock=null, primaryNodeIndex=0, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]]}
2016-12-01 16:25:33,695 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{UCState=UNDER_RECOVERY, truncateBlock=null, primaryNodeIndex=0, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]]} size 83
2016-12-01 16:25:33,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(oldBlock=BP-1922907999-127.0.1.1-1480588739936:blk_1073741866_1042, newgenerationstamp=1050, newlength=83, newtargets=[127.0.0.1:50010], closeFile=true, deleteBlock=false)
2016-12-01 16:25:33,713 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(oldBlock=BP-1922907999-127.0.1.1-1480588739936:blk_1073741866_1042, file=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582..meta.1480603499407.meta, newgenerationstamp=1050, newlength=83, newtargets=[127.0.0.1:50010]) successful
2016-12-01 16:25:36,339 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:33724 Call#55 Retry#0
2016-12-01 16:25:36,484 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 127.0.0.1:33724 Call#62 Retry#0
2016-12-01 16:25:36,670 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480605936643.meta
2016-12-01 16:25:36,701 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480605936643.meta for DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:37,487 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/37.seqid is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:38,191 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000009.log
2016-12-01 16:25:38,474 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000009.log for DFSClient_NONMAPREDUCE_38993522_1
2016-12-01 16:25:38,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: recoverLease: [Lease.  Holder: DFSClient_NONMAPREDUCE_-418518612_1, pendingcreates: 1], src=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387 from client DFSClient_NONMAPREDUCE_-418518612_1
2016-12-01 16:25:38,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-418518612_1, pendingcreates: 1], src=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387
2016-12-01 16:25:38,915 INFO BlockStateChange: BLOCK* blk_1073741865_1041{UCState=UNDER_RECOVERY, truncateBlock=null, primaryNodeIndex=0, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]]} recovery started, primary=ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]
2016-12-01 16:25:38,915 WARN org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.internalReleaseLease: File /hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387 has not been closed. Lease recovery is in progress. RecoveryId = 1053 for block blk_1073741865_1041{UCState=UNDER_RECOVERY, truncateBlock=null, primaryNodeIndex=0, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RWR]]}
2016-12-01 16:25:39,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(oldBlock=BP-1922907999-127.0.1.1-1480588739936:blk_1073741865_1041, newgenerationstamp=1053, newlength=39007344, newtargets=[127.0.0.1:50010], closeFile=true, deleteBlock=false)
2016-12-01 16:25:39,688 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(oldBlock=BP-1922907999-127.0.1.1-1480588739936:blk_1073741865_1041, file=/hbase/WALs/vm-ubuntu,16201,1480592691582-splitting/vm-ubuntu%2C16201%2C1480592691582.default.1480603497387, newgenerationstamp=1053, newlength=39007344, newtargets=[127.0.0.1:50010]) successful
2016-12-01 16:25:43,186 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/0000000000000002388.temp
2016-12-01 16:25:46,110 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:25:46,115 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/0000000000000002388.temp is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:46,827 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/0000000000000000004.temp
2016-12-01 16:25:46,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:25:46,985 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/0000000000000000004.temp is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:47,673 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/257ce2db58fdfdced67acdfcad763c88/recovered.edits/12.seqid is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:53,268 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/.tmp/6ea2b2687ac4425187cddf445426a88e
2016-12-01 16:25:54,721 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741878_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:25:54,727 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/.tmp/6ea2b2687ac4425187cddf445426a88e is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:54,815 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741877_1055 127.0.0.1:50010 
2016-12-01 16:25:54,817 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741876_1054 127.0.0.1:50010 
2016-12-01 16:25:54,835 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/default/connections/dd68ae8416f8ca8488ce38bd51d5a8db/recovered.edits/2540.seqid is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:25:56,619 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741876_1054, blk_1073741877_1055]
2016-12-01 16:25:57,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 139
2016-12-01 16:25:57,492 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000009.log is closed by DFSClient_NONMAPREDUCE_38993522_1
2016-12-01 16:25:57,506 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1052 127.0.0.1:50010 
2016-12-01 16:25:59,620 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741875_1052]
2016-12-01 16:33:04,437 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 81 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 14 Number of syncs: 59 SyncTimes(ms): 80 
2016-12-01 16:33:04,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/7febfa2fa04644a9838bd355796943b3
2016-12-01 16:33:04,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741879_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:33:04,475 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/7febfa2fa04644a9838bd355796943b3 is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:33:04,618 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/2aba5f0b31184539969040e6246c788c
2016-12-01 16:33:04,633 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741880_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 16:33:04,636 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/2aba5f0b31184539969040e6246c788c is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 16:36:27,464 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 105 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 14 Number of syncs: 75 SyncTimes(ms): 101 
2016-12-01 16:36:27,466 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741866_1050 127.0.0.1:50010 
2016-12-01 16:36:27,469 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741852_1028 127.0.0.1:50010 
2016-12-01 16:36:27,472 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741865_1053 127.0.0.1:50010 
2016-12-01 16:36:29,964 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741865_1053, blk_1073741866_1050, blk_1073741852_1028]
2016-12-01 16:38:27,486 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 108 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 14 Number of syncs: 78 SyncTimes(ms): 106 
2016-12-01 16:38:27,488 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741848_1024 127.0.0.1:50010 
2016-12-01 16:38:27,491 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741879_1057 127.0.0.1:50010 
2016-12-01 16:38:27,494 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 127.0.0.1:50010 
2016-12-01 16:38:30,009 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741879_1057, blk_1073741864_1040, blk_1073741848_1024]
2016-12-01 17:12:16,942 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: [Lease.  Holder: DFSClient_NONMAPREDUCE_-1573454821_1, pendingcreates: 1] has expired hard limit
2016-12-01 17:12:16,942 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-1573454821_1, pendingcreates: 1], src=/tmp/hadoop-yarn/staging/jlejeune/.staging/job_1480604743198_0001/job_1480604743198_0001_1.jhist
2016-12-01 17:12:16,942 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file closed.
2016-12-01 17:13:28,887 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 17:13:28,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 17:13:28,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 520
2016-12-01 17:13:28,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 117 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 14 Number of syncs: 86 SyncTimes(ms): 117 
2016-12-01 17:13:28,889 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 117 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 14 Number of syncs: 87 SyncTimes(ms): 119 
2016-12-01 17:13:28,890 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000520 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000520-0000000000000000636
2016-12-01 17:13:28,890 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 637
2016-12-01 17:13:28,961 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 1000,00 KB/s
2016-12-01 17:13:28,961 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000636 size 5282 bytes.
2016-12-01 17:13:28,965 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 519
2016-12-01 17:13:28,967 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000451, cpktTxId=0000000000000000451)
2016-12-01 17:25:28,626 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-12-01 17:25:28,630 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747.default.1480609528618
2016-12-01 17:25:28,641 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747.default.1480609528618 for DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 17:25:28,655 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-01 17:25:28,657 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747.default.1480605927878 is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 17:25:36,869 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480609536861.meta
2016-12-01 17:25:36,874 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480609536861.meta for DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 17:25:36,882 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741874_1051{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480605936643.meta
2016-12-01 17:25:36,882 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1051{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 1666
2016-12-01 17:25:37,285 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1480605920747/vm-ubuntu%2C16201%2C1480605920747..meta.1480605936643.meta is closed by DFSClient_NONMAPREDUCE_1215434946_1
2016-12-01 17:25:57,507 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000010.log
2016-12-01 17:25:57,517 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741883_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-c37bc4a3-1a8f-43b6-a0f5-071ab0e9f4e1:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-01 17:25:57,519 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000010.log is closed by DFSClient_NONMAPREDUCE_38993522_1
2016-12-01 17:25:57,523 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1061 127.0.0.1:50010 
2016-12-01 17:25:57,769 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741883_1061]
2016-12-01 17:36:27,551 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 24 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 18 SyncTimes(ms): 22 
2016-12-01 17:36:27,553 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1051 127.0.0.1:50010 
2016-12-01 17:36:27,556 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741873_1049 127.0.0.1:50010 
2016-12-01 17:36:27,978 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741873_1049, blk_1073741874_1051]
2016-12-01 18:13:29,696 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-01 18:13:29,697 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-01 18:13:29,697 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 637
2016-12-01 18:13:29,697 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 20 SyncTimes(ms): 25 
2016-12-01 18:13:29,706 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 1 Number of syncs: 21 SyncTimes(ms): 34 
2016-12-01 18:13:29,707 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000637 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000637-0000000000000000662
2016-12-01 18:13:29,707 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 663
2016-12-01 18:13:29,759 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1666,67 KB/s
2016-12-01 18:13:29,759 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000662 size 5282 bytes.
2016-12-01 18:13:29,763 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 636
2016-12-01 18:13:29,764 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000519, cpktTxId=0000000000000000519)
2016-12-01 18:17:24,643 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-12-01 18:17:24,646 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2016-12-20 15:58:00,071 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2016-12-20 15:58:00,093 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2016-12-20 15:58:00,103 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2016-12-20 15:58:00,502 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2016-12-20 15:58:00,655 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2016-12-20 15:58:00,655 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2016-12-20 15:58:00,658 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2016-12-20 15:58:00,659 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2016-12-20 15:58:00,884 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2016-12-20 15:58:00,964 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2016-12-20 15:58:00,973 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2016-12-20 15:58:00,979 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2016-12-20 15:58:00,988 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2016-12-20 15:58:00,991 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2016-12-20 15:58:00,991 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2016-12-20 15:58:00,991 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2016-12-20 15:58:01,156 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2016-12-20 15:58:01,159 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2016-12-20 15:58:01,181 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2016-12-20 15:58:01,181 INFO org.mortbay.log: jetty-6.1.26
2016-12-20 15:58:01,319 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-12-20 15:58:01,344 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-20 15:58:01,344 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2016-12-20 15:58:01,393 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2016-12-20 15:58:01,393 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2016-12-20 15:58:01,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2016-12-20 15:58:01,430 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2016-12-20 15:58:01,431 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2016-12-20 15:58:01,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2016 déc. 20 15:58:01
2016-12-20 15:58:01,433 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2016-12-20 15:58:01,433 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-20 15:58:01,435 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2016-12-20 15:58:01,435 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2016-12-20 15:58:01,442 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2016-12-20 15:58:01,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2016-12-20 15:58:01,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2016-12-20 15:58:01,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2016-12-20 15:58:01,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2016-12-20 15:58:01,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2016-12-20 15:58:01,451 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2016-12-20 15:58:01,514 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2016-12-20 15:58:01,514 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-20 15:58:01,515 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2016-12-20 15:58:01,515 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2016-12-20 15:58:01,516 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2016-12-20 15:58:01,516 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2016-12-20 15:58:01,516 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2016-12-20 15:58:01,516 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2016-12-20 15:58:01,524 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2016-12-20 15:58:01,524 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-20 15:58:01,524 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2016-12-20 15:58:01,524 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2016-12-20 15:58:01,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2016-12-20 15:58:01,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2016-12-20 15:58:01,526 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2016-12-20 15:58:01,530 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2016-12-20 15:58:01,530 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2016-12-20 15:58:01,530 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2016-12-20 15:58:01,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2016-12-20 15:58:01,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2016-12-20 15:58:01,534 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2016-12-20 15:58:01,534 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2016-12-20 15:58:01,534 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2016-12-20 15:58:01,534 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2016-12-20 15:58:01,548 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 2396@vm-ubuntu
2016-12-20 15:58:01,610 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2016-12-20 15:58:01,611 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2016-12-20 15:58:01,611 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2016-12-20 15:58:01,648 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2016-12-20 15:58:01,698 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2016-12-20 15:58:01,699 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2016-12-20 15:58:01,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2016-12-20 15:58:01,706 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2016-12-20 15:58:01,812 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2016-12-20 15:58:01,813 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 274 msecs
2016-12-20 15:58:02,142 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2016-12-20 15:58:02,152 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2016-12-20 15:58:02,165 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2016-12-20 15:58:02,202 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2016-12-20 15:58:02,213 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-12-20 15:58:02,213 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2016-12-20 15:58:02,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2016-12-20 15:58:02,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2016-12-20 15:58:02,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2016-12-20 15:58:02,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2016-12-20 15:58:02,224 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2016-12-20 15:58:02,225 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2016-12-20 15:58:02,276 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2016-12-20 15:58:02,277 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2016-12-20 15:58:02,281 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2016-12-20 15:58:02,281 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2016-12-20 15:58:02,289 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2016-12-20 15:58:06,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=802eda51-0bce-4025-9b28-e2e22d6aa748, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c98f37e4-838d-4c54-908f-454be3e52fdd;nsid=2131225354;c=0) storage 802eda51-0bce-4025-9b28-e2e22d6aa748
2016-12-20 15:58:06,126 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-20 15:58:06,126 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2016-12-20 15:58:06,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2016-12-20 15:58:06,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb for DN 127.0.0.1:50010
2016-12-20 15:58:06,264 INFO BlockStateChange: BLOCK* processReport: from storage DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=802eda51-0bce-4025-9b28-e2e22d6aa748, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-c98f37e4-838d-4c54-908f-454be3e52fdd;nsid=2131225354;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2016-12-20 16:03:26,223 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 6 
2016-12-20 16:03:26,418 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.version
2016-12-20 16:03:27,052 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/.tmp/hbase.version
2016-12-20 16:03:27,052 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2016-12-20 16:03:27,088 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 7
2016-12-20 16:03:27,472 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.version is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:27,532 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/hbase.id
2016-12-20 16:03:27,549 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-20 16:03:27,552 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/hbase.id is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:27,692 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.regioninfo
2016-12-20 16:03:27,714 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-20 16:03:27,720 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.regioninfo is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:27,916 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:27,954 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001
2016-12-20 16:03:27,971 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-20 16:03:27,976 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:29,128 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023.default.1482246209035
2016-12-20 16:03:29,316 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023.default.1482246209035 for DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:03:34,119 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023..meta.1482246214101.meta
2016-12-20 16:03:34,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023..meta.1482246214101.meta for DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:03:34,819 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/recovered.edits/3.seqid is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:03:35,415 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000001.log
2016-12-20 16:03:35,449 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/MasterProcWALs/state-00000000000000000001.log for DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:35,784 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001
2016-12-20 16:03:35,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741832_1008{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001
2016-12-20 16:03:35,809 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 312
2016-12-20 16:03:36,219 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/.tmp/.tableinfo.0000000001 is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:36,250 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/.tmp/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/.regioninfo
2016-12-20 16:03:36,269 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741833_1009{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/.tmp/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/.regioninfo
2016-12-20 16:03:36,269 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 42
2016-12-20 16:03:36,680 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/.tmp/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/.regioninfo is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:37,204 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/recovered.edits/2.seqid is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:03:58,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 475
2016-12-20 16:03:58,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000001.log is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 16:03:58,519 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 127.0.0.1:50010 
2016-12-20 16:03:59,416 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741831_1007]
2016-12-20 16:10:38,368 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 87 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 10 Number of syncs: 57 SyncTimes(ms): 132 
2016-12-20 16:10:38,414 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/.tmp/eb3995471d3b462c86a5ab9060fc5089
2016-12-20 16:10:38,427 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-20 16:10:38,429 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/namespace/5d22ff182738e1d7a365fbb26750bd4f/.tmp/eb3995471d3b462c86a5ab9060fc5089 is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:10:38,565 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/data/hbase/meta/1588230740/.tmp/48a41ab9bc4743b1bb147978f4bf9508
2016-12-20 16:10:38,576 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 0
2016-12-20 16:10:38,579 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/data/hbase/meta/1588230740/.tmp/48a41ab9bc4743b1bb147978f4bf9508 is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 16:31:11,449 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2016-12-20 16:31:11,449 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2016-12-20 16:31:11,449 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2016-12-20 16:31:11,449 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 101 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 10 Number of syncs: 67 SyncTimes(ms): 141 
2016-12-20 16:31:11,453 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 101 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 10 Number of syncs: 68 SyncTimes(ms): 144 
2016-12-20 16:31:11,456 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000101
2016-12-20 16:31:11,466 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 102
2016-12-20 16:31:12,594 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 1000,00 KB/s
2016-12-20 16:31:12,595 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000101 size 3479 bytes.
2016-12-20 16:31:12,599 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2016-12-20 17:03:29,828 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2016-12-20 17:03:29,839 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023.default.1482249809803
2016-12-20 17:03:29,853 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023.default.1482249809803 for DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 17:03:29,863 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-20 17:03:29,865 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023.default.1482246209035 is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 17:03:34,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023..meta.1482249814466.meta
2016-12-20 17:03:34,500 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023..meta.1482249814466.meta for DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 17:03:34,509 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 83
2016-12-20 17:03:34,511 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/WALs/vm-ubuntu,16201,1482246204023/vm-ubuntu%2C16201%2C1482246204023..meta.1482246214101.meta is closed by DFSClient_NONMAPREDUCE_545024984_1
2016-12-20 17:03:58,551 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} for /hbase/MasterProcWALs/state-00000000000000000002.log
2016-12-20 17:03:58,566 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741838_1014{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /hbase/MasterProcWALs/state-00000000000000000002.log
2016-12-20 17:03:58,566 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-79ed92d1-19ed-4a0e-8720-02bc49b580bb:NORMAL:127.0.0.1:50010|RBW]]} size 30
2016-12-20 17:03:58,972 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /hbase/MasterProcWALs/state-00000000000000000002.log is closed by DFSClient_NONMAPREDUCE_1531708021_1
2016-12-20 17:03:58,980 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 127.0.0.1:50010 
2016-12-20 17:04:01,044 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741838_1014]
2016-12-20 17:14:28,584 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 26 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 1 Number of syncs: 18 SyncTimes(ms): 37 
2016-12-20 17:14:28,587 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 127.0.0.1:50010 
2016-12-20 17:14:28,591 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 127.0.0.1:50010 
2016-12-20 17:14:31,396 INFO BlockStateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741829_1005, blk_1073741830_1006]
2016-12-20 17:22:09,195 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2016-12-20 17:22:09,200 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2017-01-05 12:37:36,778 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2017-01-05 12:37:36,789 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-05 12:37:36,793 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-01-05 12:37:37,174 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-05 12:37:37,293 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-05 12:37:37,293 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-01-05 12:37:37,296 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2017-01-05 12:37:37,297 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2017-01-05 12:37:37,650 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-01-05 12:37:37,823 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-05 12:37:37,833 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-05 12:37:37,841 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-01-05 12:37:37,849 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-05 12:37:37,853 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-01-05 12:37:37,853 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-05 12:37:37,853 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-05 12:37:38,049 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-01-05 12:37:38,052 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-01-05 12:37:38,073 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-01-05 12:37:38,073 INFO org.mortbay.log: jetty-6.1.26
2017-01-05 12:37:38,228 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-01-05 12:37:38,262 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-05 12:37:38,262 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-05 12:37:38,309 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-05 12:37:38,309 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-05 12:37:38,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-05 12:37:38,348 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-05 12:37:38,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-05 12:37:38,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 janv. 05 12:37:38
2017-01-05 12:37:38,351 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-05 12:37:38,351 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 12:37:38,353 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-05 12:37:38,353 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-05 12:37:38,358 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-05 12:37:38,359 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-05 12:37:38,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2017-01-05 12:37:38,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-05 12:37:38,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-05 12:37:38,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-05 12:37:38,369 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-05 12:37:38,421 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-05 12:37:38,421 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 12:37:38,421 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-05 12:37:38,421 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-05 12:37:38,422 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-05 12:37:38,422 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-05 12:37:38,422 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-05 12:37:38,422 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-05 12:37:38,429 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-05 12:37:38,429 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 12:37:38,429 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-05 12:37:38,429 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-05 12:37:38,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-05 12:37:38,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-05 12:37:38,430 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-05 12:37:38,434 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-05 12:37:38,434 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-05 12:37:38,434 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-05 12:37:38,435 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-01-05 12:37:38,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-01-05 12:37:38,437 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-01-05 12:37:38,438 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 12:37:38,438 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-01-05 12:37:38,438 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-01-05 12:37:38,452 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 3819@vm-ubuntu
2017-01-05 12:37:38,511 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2017-01-05 12:37:38,511 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-01-05 12:37:38,511 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-05 12:37:38,557 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-05 12:37:38,606 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-05 12:37:38,606 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2017-01-05 12:37:38,613 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-01-05 12:37:38,614 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-01-05 12:37:38,755 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-05 12:37:38,755 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 314 msecs
2017-01-05 12:37:39,070 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-01-05 12:37:39,080 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-05 12:37:39,091 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-01-05 12:37:39,135 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-01-05 12:37:39,145 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-05 12:37:39,146 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-05 12:37:39,146 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-01-05 12:37:39,146 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-01-05 12:37:39,146 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-01-05 12:37:39,146 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-01-05 12:37:39,156 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-01-05 12:37:39,166 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec
2017-01-05 12:37:39,204 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-05 12:37:39,205 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-01-05 12:37:39,207 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-01-05 12:37:39,207 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-01-05 12:37:39,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-01-05 12:37:43,488 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=ed9b7462-7992-4034-8354-7b5d9b1c2f88, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1906cd4e-67c0-4501-a80d-67e5f6ce6ecb;nsid=715667178;c=0) storage ed9b7462-7992-4034-8354-7b5d9b1c2f88
2017-01-05 12:37:43,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 12:37:43,491 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-01-05 12:37:43,619 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 12:37:43,619 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733 for DN 127.0.0.1:50010
2017-01-05 12:37:43,672 INFO BlockStateChange: BLOCK* processReport: from storage DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=ed9b7462-7992-4034-8354-7b5d9b1c2f88, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1906cd4e-67c0-4501-a80d-67e5f6ce6ecb;nsid=715667178;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2017-01-05 12:38:47,718 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-05 12:38:47,718 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-05 12:38:47,718 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-01-05 12:38:47,719 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 11 
2017-01-05 12:38:47,725 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 17 
2017-01-05 12:38:47,726 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000002
2017-01-05 12:38:47,728 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2017-01-05 12:38:48,913 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2017-01-05 12:38:48,913 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 355 bytes.
2017-01-05 12:38:48,923 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-01-05 12:43:32,721 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2017-01-05 12:43:32,783 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733:NORMAL:127.0.0.1:50010|RBW]]} for /mat4_4.txt._COPYING_
2017-01-05 12:43:33,127 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733:NORMAL:127.0.0.1:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /mat4_4.txt._COPYING_
2017-01-05 12:43:33,127 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-01-05 12:43:33,153 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733:NORMAL:127.0.0.1:50010|RBW]]} size 48
2017-01-05 12:43:33,545 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /mat4_4.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-188136133_1
2017-01-05 13:38:49,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-05 13:38:49,903 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-05 13:38:49,903 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2017-01-05 13:38:49,904 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 18 
2017-01-05 13:38:49,907 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 8 Total time for transactions(ms): 21 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 21 
2017-01-05 13:38:49,908 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000003-0000000000000000010
2017-01-05 13:38:49,909 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 11
2017-01-05 13:38:50,017 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2017-01-05 13:38:50,017 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000010 size 444 bytes.
2017-01-05 13:38:50,020 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2017-01-05 13:38:50,020 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-05 13:54:44,847 INFO BlockStateChange: BLOCK* processReport: from storage DS-4955e3cd-dfcf-4683-a5a1-f48c001ec733 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=ed9b7462-7992-4034-8354-7b5d9b1c2f88, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-1906cd4e-67c0-4501-a80d-67e5f6ce6ecb;nsid=715667178;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2017-01-05 13:58:06,948 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-05 13:58:06,949 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2017-01-05 17:19:27,775 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2017-01-05 17:19:27,785 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-05 17:19:27,790 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-01-05 17:19:28,167 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-05 17:19:28,290 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-05 17:19:28,291 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-01-05 17:19:28,294 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2017-01-05 17:19:28,295 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2017-01-05 17:19:29,212 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-01-05 17:19:29,291 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-05 17:19:29,300 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-05 17:19:29,308 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-01-05 17:19:29,316 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-05 17:19:29,318 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-01-05 17:19:29,318 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-05 17:19:29,318 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-05 17:19:29,518 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-01-05 17:19:29,521 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-01-05 17:19:29,538 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-01-05 17:19:29,538 INFO org.mortbay.log: jetty-6.1.26
2017-01-05 17:19:29,686 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-01-05 17:19:29,718 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-05 17:19:29,718 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-05 17:19:29,758 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-05 17:19:29,759 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-05 17:19:29,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-05 17:19:29,796 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-05 17:19:29,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-05 17:19:29,797 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 janv. 05 17:19:29
2017-01-05 17:19:29,799 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-05 17:19:29,799 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 17:19:29,801 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-05 17:19:29,801 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-05 17:19:29,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-05 17:19:29,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-05 17:19:29,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-05 17:19:29,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-05 17:19:29,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-05 17:19:29,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-05 17:19:29,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-05 17:19:29,808 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-05 17:19:29,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2017-01-05 17:19:29,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-05 17:19:29,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-05 17:19:29,815 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-05 17:19:29,818 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-05 17:19:29,897 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-05 17:19:29,897 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 17:19:29,898 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-05 17:19:29,898 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-05 17:19:29,899 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-05 17:19:29,899 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-05 17:19:29,899 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-05 17:19:29,899 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-05 17:19:29,907 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-05 17:19:29,907 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 17:19:29,907 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-05 17:19:29,907 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-05 17:19:29,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-05 17:19:29,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-05 17:19:29,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-05 17:19:29,912 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-05 17:19:29,912 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-05 17:19:29,912 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-05 17:19:29,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-01-05 17:19:29,917 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-01-05 17:19:29,919 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-01-05 17:19:29,919 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-05 17:19:29,919 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-01-05 17:19:29,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-01-05 17:19:29,946 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 5308@vm-ubuntu
2017-01-05 17:19:30,014 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2017-01-05 17:19:30,014 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-01-05 17:19:30,015 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-05 17:19:30,050 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-05 17:19:30,077 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-05 17:19:30,077 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2017-01-05 17:19:30,085 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-01-05 17:19:30,085 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-01-05 17:19:30,200 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-05 17:19:30,200 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 277 msecs
2017-01-05 17:19:30,572 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-01-05 17:19:30,581 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-05 17:19:30,595 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-01-05 17:19:30,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-01-05 17:19:30,638 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-05 17:19:30,639 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-05 17:19:30,639 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-01-05 17:19:30,639 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-01-05 17:19:30,639 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-01-05 17:19:30,639 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-01-05 17:19:30,651 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-01-05 17:19:30,669 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 29 msec
2017-01-05 17:19:30,709 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-05 17:19:30,710 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-01-05 17:19:30,712 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-01-05 17:19:30,712 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-01-05 17:19:30,726 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-01-05 17:19:34,580 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=984dcdd0-965a-43e4-8123-5aaf3515a355, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-59ccc7e0-ff98-409b-b161-aa92378f33e5;nsid=341068095;c=0) storage 984dcdd0-965a-43e4-8123-5aaf3515a355
2017-01-05 17:19:34,580 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 17:19:34,583 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-01-05 17:19:34,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-05 17:19:34,681 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-15422dc1-628b-413c-990b-cc3e98562387 for DN 127.0.0.1:50010
2017-01-05 17:19:34,734 INFO BlockStateChange: BLOCK* processReport: from storage DS-15422dc1-628b-413c-990b-cc3e98562387 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=984dcdd0-965a-43e4-8123-5aaf3515a355, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-59ccc7e0-ff98-409b-b161-aa92378f33e5;nsid=341068095;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-01-05 17:20:39,100 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-05 17:20:39,100 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-05 17:20:39,100 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-01-05 17:20:39,100 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2017-01-05 17:20:39,101 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 9 
2017-01-05 17:20:39,103 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000002
2017-01-05 17:20:39,106 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2017-01-05 17:20:40,551 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2017-01-05 17:20:40,551 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 355 bytes.
2017-01-05 17:20:40,555 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-01-05 17:20:47,423 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} for /in._COPYING_
2017-01-05 17:20:49,715 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} for /in._COPYING_
2017-01-05 17:20:49,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} size 134217728
2017-01-05 17:20:50,473 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-05 17:20:50,493 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /in._COPYING_ is closed by DFSClient_NONMAPREDUCE_-920952297_1
2017-01-05 17:23:40,260 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 11 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 0 Number of syncs: 7 SyncTimes(ms): 39 
2017-01-05 17:23:40,352 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-05 17:23:42,264 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-05 17:23:42,277 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-05 17:23:44,177 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-05 17:23:44,181 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-05 17:23:44,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-15422dc1-628b-413c-990b-cc3e98562387:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-05 17:23:44,718 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /in2._COPYING_ is closed by DFSClient_NONMAPREDUCE_-57606776_1
2017-01-05 18:20:41,384 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-05 18:20:41,385 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-05 18:20:41,385 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 3
2017-01-05 18:20:41,385 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 23 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 0 Number of syncs: 13 SyncTimes(ms): 121 
2017-01-05 18:20:41,387 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 23 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 0 Number of syncs: 14 SyncTimes(ms): 123 
2017-01-05 18:20:41,388 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000003 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000003-0000000000000000025
2017-01-05 18:20:41,396 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 26
2017-01-05 18:20:41,541 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2017-01-05 18:20:41,541 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000025 size 553 bytes.
2017-01-05 18:20:41,545 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2017-01-05 18:20:41,546 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-05 18:44:30,050 INFO BlockStateChange: BLOCK* processReport: from storage DS-15422dc1-628b-413c-990b-cc3e98562387 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=984dcdd0-965a-43e4-8123-5aaf3515a355, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-59ccc7e0-ff98-409b-b161-aa92378f33e5;nsid=341068095;c=0), blocks: 5, hasStaleStorage: false, processing time: 2 msecs
2017-01-05 19:08:25,871 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-05 19:08:25,873 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
2017-01-06 14:05:54,928 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
2017-01-06 14:05:54,939 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-01-06 14:05:54,943 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-01-06 14:05:55,353 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-01-06 14:05:55,517 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-01-06 14:05:55,517 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-01-06 14:05:55,520 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2017-01-06 14:05:55,521 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2017-01-06 14:05:55,831 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-01-06 14:05:55,978 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-01-06 14:05:55,986 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-01-06 14:05:55,994 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-01-06 14:05:56,001 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-01-06 14:05:56,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-01-06 14:05:56,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-01-06 14:05:56,004 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-01-06 14:05:56,176 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-01-06 14:05:56,180 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-01-06 14:05:56,204 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-01-06 14:05:56,205 INFO org.mortbay.log: jetty-6.1.26
2017-01-06 14:05:56,401 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-01-06 14:05:56,434 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-06 14:05:56,434 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-01-06 14:05:56,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-01-06 14:05:56,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-01-06 14:05:56,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-01-06 14:05:56,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-01-06 14:05:56,520 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-01-06 14:05:56,521 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 janv. 06 14:05:56
2017-01-06 14:05:56,522 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-01-06 14:05:56,522 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-06 14:05:56,524 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-01-06 14:05:56,524 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-01-06 14:05:56,530 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-01-06 14:05:56,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jlejeune (auth:SIMPLE)
2017-01-06 14:05:56,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-01-06 14:05:56,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-01-06 14:05:56,538 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-01-06 14:05:56,539 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-01-06 14:05:56,610 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-01-06 14:05:56,610 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-06 14:05:56,610 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-01-06 14:05:56,610 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-01-06 14:05:56,611 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-01-06 14:05:56,611 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-01-06 14:05:56,611 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-01-06 14:05:56,611 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-01-06 14:05:56,618 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-01-06 14:05:56,618 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-06 14:05:56,618 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-01-06 14:05:56,618 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-01-06 14:05:56,619 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-01-06 14:05:56,619 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-01-06 14:05:56,620 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-01-06 14:05:56,623 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-01-06 14:05:56,623 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-01-06 14:05:56,623 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-01-06 14:05:56,624 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-01-06 14:05:56,625 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-01-06 14:05:56,627 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-01-06 14:05:56,627 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-01-06 14:05:56,627 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-01-06 14:05:56,627 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-01-06 14:05:56,643 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jlejeune/dfs/name/in_use.lock acquired by nodename 4639@vm-ubuntu
2017-01-06 14:05:56,686 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jlejeune/dfs/name/current
2017-01-06 14:05:56,686 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-01-06 14:05:56,687 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-06 14:05:56,725 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-01-06 14:05:56,773 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-01-06 14:05:56,773 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000
2017-01-06 14:05:56,782 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-01-06 14:05:56,782 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-01-06 14:05:56,900 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-01-06 14:05:56,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 270 msecs
2017-01-06 14:05:57,234 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2017-01-06 14:05:57,246 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-01-06 14:05:57,260 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-01-06 14:05:57,292 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-01-06 14:05:57,302 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-06 14:05:57,302 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-01-06 14:05:57,302 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-01-06 14:05:57,303 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-01-06 14:05:57,303 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-01-06 14:05:57,303 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-01-06 14:05:57,310 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-01-06 14:05:57,328 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2017-01-06 14:05:57,361 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-01-06 14:05:57,361 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-01-06 14:05:57,367 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2017-01-06 14:05:57,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-01-06 14:05:57,381 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-01-06 14:06:01,127 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:50010, datanodeUuid=888cb4da-f832-4bae-9e23-42a1dd9f9360, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a93796b5-1dc9-42f3-b3b9-1248494359a6;nsid=1856733022;c=0) storage 888cb4da-f832-4bae-9e23-42a1dd9f9360
2017-01-06 14:06:01,128 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-06 14:06:01,129 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2017-01-06 14:06:01,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-01-06 14:06:01,240 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-697a97d4-438b-4c3f-9450-67f3b3fc1668 for DN 127.0.0.1:50010
2017-01-06 14:06:01,307 INFO BlockStateChange: BLOCK* processReport: from storage DS-697a97d4-438b-4c3f-9450-67f3b3fc1668 node DatanodeRegistration(127.0.0.1:50010, datanodeUuid=888cb4da-f832-4bae-9e23-42a1dd9f9360, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-a93796b5-1dc9-42f3-b3b9-1248494359a6;nsid=1856733022;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-01-06 14:06:29,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-06 14:06:30,502 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-06 14:06:30,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} size 134217728
2017-01-06 14:06:31,963 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-06 14:06:31,966 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} for /in2._COPYING_
2017-01-06 14:06:32,222 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-697a97d4-438b-4c3f-9450-67f3b3fc1668:NORMAL:127.0.0.1:50010|RBW]]} size 0
2017-01-06 14:06:32,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /in2._COPYING_ is closed by DFSClient_NONMAPREDUCE_125776726_1
2017-01-06 14:07:10,983 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-06 14:07:10,983 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-06 14:07:10,983 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-01-06 14:07:10,983 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 14 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 8 SyncTimes(ms): 26 
2017-01-06 14:07:10,985 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 14 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 0 Number of syncs: 9 SyncTimes(ms): 28 
2017-01-06 14:07:10,986 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000001-0000000000000000014
2017-01-06 14:07:10,996 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 15
2017-01-06 14:07:13,097 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,01s at 0,00 KB/s
2017-01-06 14:07:13,098 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000014 size 473 bytes.
2017-01-06 14:07:13,117 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-01-06 15:07:14,124 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2017-01-06 15:07:14,130 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-01-06 15:07:14,131 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 15
2017-01-06 15:07:14,134 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2017-01-06 15:07:14,137 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 8 
2017-01-06 15:07:14,141 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jlejeune/dfs/name/current/edits_inprogress_0000000000000000015 -> /tmp/hadoop-jlejeune/dfs/name/current/edits_0000000000000000015-0000000000000000016
2017-01-06 15:07:14,143 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 17
2017-01-06 15:07:14,248 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0,00s at 0,00 KB/s
2017-01-06 15:07:14,248 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000016 size 473 bytes.
2017-01-06 15:07:14,251 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 14
2017-01-06 15:07:14,251 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jlejeune/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-01-06 15:21:22,754 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-01-06 15:21:22,758 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
